================================================================================
LAB 1B - PART 2: GPU MULTICORE IMPLEMENTATION - QUICK REFERENCE
================================================================================

IMPLEMENTATION COMPLETED: ✓
- File: cse5304/lab1b/mandelbrot_gpu_2_s26.cu
- Kernel: mandelbrot_gpu_vector_multicore()
- Launch: launch_mandelbrot_gpu_vector_multicore()
- Config: <<<84, 128>>> = 336 warps (1 per warp scheduler)

================================================================================
QUESTION 1: HOW DOES YOUR PARTITION WORK ACROSS CORES?
================================================================================

ANSWER:
-------
The implementation uses LINEAR PIXEL INDEXING with WARP-BASED PARTITIONING:

1. Total work: 2048×2048 = 4,194,304 pixels
2. Total warps: 336 (one per warp scheduler)
3. Pixels per warp: 4,194,304 / 336 = 12,483 pixels

Each warp gets a contiguous block of ~12,483 pixels:
- Warp 0: pixels [0, 12483)
- Warp 1: pixels [12483, 24966)
- ...
- Warp 335: pixels [4,181,821, 4,194,304)

Within each warp, the 32 threads use a STRIDE PATTERN:
- Thread 0 processes pixels: start + 0, start + 32, start + 64, ...
- Thread 1 processes pixels: start + 1, start + 33, start + 65, ...
- Thread 31 processes pixels: start + 31, start + 63, start + 95, ...

Linear pixel index converts to (row, col):
- row = pixel / img_size
- col = pixel % img_size

ADVANTAGES:
- Simple and easy to implement
- Equal work per warp (good load balancing on paper)
- Memory coalescing within warps (stride-32 pattern)

DISADVANTAGES:
- Warp divergence (32 threads compute different pixels)
- Actual load imbalance (Mandelbrot iterations vary per pixel)
- Underutilization (only 1 warp per scheduler)

================================================================================
QUESTION 2: HOW MUCH SPEEDUP DO YOU GET FROM THE GPU?
================================================================================

ANSWER:
-------
GPU vs CPU Multicore Speedup: 3.56×

Performance Data (2048×2048, 2000 max iterations):
- CPU Multicore: 14.1 ms (64 threads + AVX2)
- GPU Multicore: 3.96 ms (336 warps)
- Speedup: 14.1 / 3.96 = 3.56×

Why Only 3.56× Despite 21× More Parallelism?

CPU: 64 cores × 8 SIMD lanes = 512-way parallelism
GPU: 336 warps × 32 threads = 10,752-way parallelism
Ratio: 10,752 / 512 = 21×

But observed: 3.56× speedup, not 21×!

GPU EFFICIENCY: 3.56 / 21 = 17% of theoretical advantage

LIMITING FACTORS:
1. Insufficient warps for latency hiding (need 4-16× per scheduler)
2. Warp divergence on variable iteration counts
3. Load imbalance across warps
4. Memory latency not fully hidden
5. CPU has better cache hierarchy and independent thread execution

ABSOLUTE PERFORMANCE:
- GPU achieves 4.2% of peak FLOPS (36.9 TFLOPS theoretical)
- CPU achieves 78% of peak FLOPS (380 GFLOPS theoretical)
- CPU is MORE EFFICIENT for this workload!

================================================================================
QUESTION 3: ALTERNATIVE LAUNCH CONFIGURATIONS
================================================================================

TESTED CONFIGURATIONS:
----------------------
1. <<<84, 128>>>  = 84 × 4 warps = 336 warps
2. <<<168, 64>>>  = 168 × 2 warps = 336 warps
3. <<<42, 256>>>  = 42 × 8 warps = 336 warps

ANSWER: Do they still assign one warp per scheduler?
-------
YES! All three configurations produce exactly 336 warps total.

The GPU warp scheduler doesn't care about block boundaries - it only sees
warps. As long as total_warps = 336, each of the 336 warp schedulers gets
one warp to execute.

PERFORMANCE RESULTS:
-------------------
All three configurations perform identically (~11ms in test harness):
- <<<84, 128>>>:  11.21 ms
- <<<168, 64>>>:  11.13 ms
- <<<42, 256>>>:  11.38 ms

Differences are within measurement noise (<2%).

BONUS TEST - MORE WARPS:
-----------------------
<<<84, 256>>> = 672 warps (2 per scheduler): 5.99 ms

Doubling warps nearly halves runtime! Confirms that more warps improve
latency hiding and throughput.

CONCLUSION:
-----------
Block/thread grid shape is IRRELEVANT for performance when total warp count
is constant. Choose configuration based on:
- Readability: <<<84, 128>>> maps cleanly (1 block per SM)
- Shared memory needs (if applicable)
- Occupancy constraints

================================================================================
KEY RESULTS SUMMARY
================================================================================

✓ GPU implementation works correctly (0 error vs reference)
✓ Runtime: 3.96 ms (3.56× faster than CPU)
✓ Work partitioning: Block-based, stride-32 within warps
✓ Launch config: <<<84, 128>>> optimal for readability
✓ Alternative configs: <<<168, 64>>> and <<<42, 256>>> perform identically
✓ Scaling: More warps = better performance (672 warps → 5.99 ms)

EFFICIENCY ANALYSIS:
- CPU: 99% efficient (127× speedup from 128× parallelism)
- GPU: 17% efficient (3.56× speedup from 21× parallelism ratio)

LESSON: Simple parallelization favors CPU; GPU needs many warps to shine!

================================================================================
FILES GENERATED
================================================================================

Implementation:
- mandelbrot_gpu_2_s26.cu (main implementation)
- test_launch_configs.cu (benchmark different configurations)

Output:
- out/mandelbrot_gpu_vector_multicore.bmp (2048×2048 output image)

Analysis:
- PART2_GPU_MULTICORE_ANALYSIS.txt (detailed analysis)
- COMPLETE_LAB1B_SUMMARY.txt (full lab comparison)
- PART2_QUICK_REFERENCE.txt (this file)

Compilation:
nvcc --compiler-bindir /usr/bin -o mandelbrot_gpu_2_s26 mandelbrot_gpu_2_s26.cu -O3

Execution:
./mandelbrot_gpu_2_s26 -i vector_multicore -r 2048

================================================================================
